name: Twitter Scraper

on:
  repository_dispatch:
    types: [start-scrape]

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    # Use a container with Node.js and Puppeteer pre-installed
    container:
      image: ghcr.io/puppeteer/puppeteer:20.9.0
      options: --shm-size=2gb

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      # Simpler caching strategy with fewer variations in keys
      - name: Cache dependencies
        uses: actions/cache@v3
        id: cache
        with:
          path: |
            **/node_modules
            ~/.npm
            ~/.cache/puppeteer
          # Use a simpler key based only on package-lock.json
          key: ${{ runner.os }}-deps-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-deps-

      # Skip installation if cache hit
      - name: Install dependencies
        if: steps.cache.outputs.cache-hit != 'true'
        run: |
          echo "Cache miss - installing dependencies..."
          npm ci
          echo "Dependencies installed successfully"

      # Skip Puppeteer setup since we're using a pre-installed container
      # Just verify Chrome is available
      - name: Verify Chrome
        run: |
          echo "Checking Chrome installation..."
          which chromium
          chromium --version
          echo "Chrome verification complete"

      - name: Run scraper
        run: node router.js --payload '${{ toJson(github.event.client_payload) }}'
        env:
          NODE_OPTIONS: "--max-old-space-size=4096"
          PUPPETEER_EXECUTABLE_PATH: "/usr/bin/chromium"
