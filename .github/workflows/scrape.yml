name: Twitter Scraper

on:
  repository_dispatch:
    types: [start-scrape]

jobs:
  scrape:
    runs-on: ubuntu-latest
    timeout-minutes: 15
    
    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v2

      - name: Cache node_modules
        uses: actions/cache@v3
        with:
          path: node_modules
          key: ${{ runner.os }}-node-${{ hashFiles('package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      # Cache Docker image using tar approach
      - name: Cache Docker image
        id: cache-docker-image
        uses: actions/cache@v3
        with:
          path: /tmp/docker-image
          key: ${{ runner.os }}-docker-image-puppeteer-20.9.0
          restore-keys: |
            ${{ runner.os }}-docker-image-puppeteer-20.9.0

      # Load the Docker image if it was cached
      - name: Load Docker image from cache
        if: steps.cache-docker-image.outputs.cache-hit == 'true'
        run: |
          mkdir -p /tmp/docker-image
          docker load < /tmp/docker-image/puppeteer-image.tar
          echo "Docker image loaded from cache"

      # Pull the image if not cached
      - name: Pull Docker image if not cached
        if: steps.cache-docker-image.outputs.cache-hit != 'true'
        run: |
          docker pull ghcr.io/puppeteer/puppeteer:20.9.0
          mkdir -p /tmp/docker-image
          docker save ghcr.io/puppeteer/puppeteer:20.9.0 > /tmp/docker-image/puppeteer-image.tar
          echo "Docker image pulled and saved to cache"

      - name: Run in Docker container
        uses: addnab/docker-run-action@v3
        with:
          image: ghcr.io/puppeteer/puppeteer:20.9.0
          options: --shm-size=2gb -v ${{ github.workspace }}:/app
          run: |
            cd /app
            
            # Install dependencies if node_modules doesn't exist
            if [ ! -d "node_modules" ]; then
              echo "Installing dependencies..."
              npm ci
              echo "Dependencies installed successfully"
            else
              echo "Using existing node_modules"
            fi
            
            # Find Chrome executable path
            echo "Finding Chrome executable..."
            CHROME_PATH=$(which chromium-browser || which chromium || which google-chrome || echo "not-found")
            echo "Chrome executable found at: $CHROME_PATH"
            
            # List available browsers
            echo "Available browsers:"
            find / -name "chromium*" -type f -executable 2>/dev/null || echo "No chromium found"
            find / -name "chrome*" -type f -executable 2>/dev/null || echo "No chrome found"
            
            # Modify the path based on what we found
            if [ "$CHROME_PATH" = "not-found" ]; then
              # Try the default path in the puppeteer image
              CHROME_PATH="/usr/bin/google-chrome-stable"
              echo "Using default path: $CHROME_PATH"
            fi
            
            # Verify Chrome executable exists
            if [ -f "$CHROME_PATH" ]; then
              echo "Chrome executable verified at: $CHROME_PATH"
              $CHROME_PATH --version
            else
              echo "ERROR: Chrome executable not found at: $CHROME_PATH"
              exit 1
            fi
            
            # Run the scraper with the correct Chrome path
            NODE_OPTIONS="--max-old-space-size=4096" PUPPETEER_EXECUTABLE_PATH="$CHROME_PATH" node router.js --payload '${{ toJson(github.event.client_payload) }}'
